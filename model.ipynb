{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d990660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_from_disk\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6967abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"tokenised_wmt14\")\n",
    "dataset = dataset.with_format(\"torch\") # turns the lists to torch tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00f544e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([ 1775,  1245, 13027,  1007,  2988,  1126,  1212,  3902,  1033,  1024,\n",
      "         3823,  1055,  1024,  2893,  2995, 17793,  3085,  1212, 24621, 28891]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([ 1754,  1948,  3633,  1074,  7316,  1484,  1500,  3747,  1165, 17168,\n",
      "         1644, 17319,  1033,  1156,  4890,  2851,  1079])}\n",
      "Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nÃ¤chsten Tagen.\n",
      "{'input_ids': tensor([3155, 1152, 6021, 9327, 1117, 1212, 9288,   10, 1014, 7415, 7938]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([ 1397,  2984,  4790,  1255,  1139,  1500,  5063,  1373,  2520,  1463,\n",
      "         1139,  1179, 12708])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokeniser = PreTrainedTokenizerFast.from_pretrained(\"./trained_tokeniser\") \n",
    "\n",
    "\n",
    "data1 =dataset[\"train\"][1]\n",
    "print(data1)\n",
    "input_ids_list = data1[\"labels\"]\n",
    "print(tokeniser.decode(input_ids_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ba3235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2449617\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2062\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1969\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a73746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition:\n",
    "import torch.nn as nn\n",
    "\n",
    "# Note torch bilstm's process all timesteps of a sequence at once.\n",
    "\n",
    "class Bilstm_Encoder(nn.Module):\n",
    "    def __init___(self,input_size,embedding_dim,hidden_size):\n",
    "        super().__init__()\n",
    "        # input_size (B,L)\n",
    "        # embedding matrix C\n",
    "        self.embedding_matrix = nn.embedding(input_size,embedding_dim)\n",
    "        # Bilstm h_n.shape = (b,L,hidden_size)\n",
    "        self.bilstm = nn.LSTM(input_size=embedding_dim, hidden_size= hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self,input): # (B,L)\n",
    "        embedded = self.embedding_matrix(input) # Returns (B,L,embedding_dim) \n",
    "        output, h_n = self.bilstm(embedded)\n",
    "        # output: (B,L,2*hidden_size) outputs h_t fwd backward concatenated for the top layer., h_n = (D*num_layers,B,hidden_size) outputs h_n fwd, backward for every layer\n",
    "        return output, h_n\n",
    "\n",
    "# compute f(hi,sj) for all hi, then softmax over.\n",
    "class Luong_attention(nn.Module):\n",
    "    def __init__(self,encoder_dim,decoder_dim): # Output = C_i = (B,2*hidden_size) \n",
    "        super().__init__()\n",
    "        self.decoder_dim= decoder_dim\n",
    "        self.encoder_dim= encoder_dim\n",
    "        self.W = nn.Parameter(torch.FloatTensor(\n",
    "            self.decoder_dim, self.encoder_dim).uniform_(-0.1, 0.1)) # (decoder,encoder)\n",
    "\n",
    "    def forward(self,query,values): # query:(B,decoder),values: (B,L,encoder_dim)\n",
    "        transpose_values = torch.transpose(values,dim0=1,dim1=2) # (B,encoder_dim,L)\n",
    "        key = self.W @ transpose_values # (B,decoder,L) broadcasting\n",
    "\n",
    "        # need to transform query from (B,decoder) -> (B,1,decoder)\n",
    "        query = query.unsqueeze(1)\n",
    "        attention_weights = query @ key # (B,1,L)\n",
    "        attention_scores = nn.Softmax(attention_weights,dim=-1) # (B,1,L)\n",
    "        context_vector = (attention_scores @ values).squeeze(1) # (B,1,L) @ (B,L,encoder_dim) = (B,1,encoder_dim)\n",
    "        return context_vector\n",
    "\n",
    "class Bilstm_Decoder(nn.Module):\n",
    "    def __init___(self,input_size,hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ab4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 46])\n",
      "torch.Size([32, 46])\n",
      "torch.Size([32, 49])\n"
     ]
    }
   ],
   "source": [
    "# The training loop\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# prepare collate_fn\n",
    "collate_fn = DataCollatorForSeq2Seq(tokeniser,padding=True)\n",
    "\n",
    "#prepare the dataloaders:\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(dataset[\"train\"], shuffle=True, batch_size=batch_size,collate_fn=collate_fn)\n",
    "for batch in train_dataloader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    print(input_ids.shape)\n",
    "    print(attention_mask.shape)\n",
    "    print(labels.shape)\n",
    "\n",
    "    break\n",
    "\n",
    "#Labels shape is different since its matched with the one to one of the decoders output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c07c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
